{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maria/NeuroGarage/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ninputs = processor(images=image, return_tensors=\"pt\")\\noutputs = model(**inputs)\\nlogits = outputs.logits\\n# model predicts one of the 1000 ImageNet classes\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "from abc import ABC, abstractmethod\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1) Load .env\n",
    "load_dotenv()\n",
    "\n",
    "class PipelineStep(ABC):\n",
    "    \"\"\"Abstract base class for pipeline steps.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def process(self, data):\n",
    "        \"\"\"Process the input data and return the transformed output.\"\"\"\n",
    "        pass\n",
    "\n",
    "class AllenAPI:\n",
    "    \"\"\"Singleton class for accessing Allen Brain Observatory API\"\"\"\n",
    "    \n",
    "    _instance = None\n",
    "    _boc = None  # Lazy-loaded BrainObservatoryCache instance\n",
    "\n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    @property\n",
    "    def boc(self):\n",
    "        \"\"\"Lazy-load BrainObservatoryCache only when accessed.\"\"\"\n",
    "        if self._boc is None:\n",
    "            allen_cache_path = os.environ.get('CAIM_ALLEN_CACHE_PATH')\n",
    "            if not allen_cache_path:\n",
    "                raise ValueError(\"AllenAPI requires a valid cache path. Set `CAIM_ALLEN_CACHE_PATH` in .env.\")\n",
    "\n",
    "            manifest_path = Path(allen_cache_path) / 'brain_observatory_manifest.json'\n",
    "            self._boc = BrainObservatoryCache(manifest_file=str(manifest_path))\n",
    "\n",
    "        return self._boc\n",
    "\n",
    "    def get_boc(self):\n",
    "        \"\"\"Retrieve the BrainObservatoryCache object, ensuring it is initialized.\"\"\"\n",
    "        return self.boc\n",
    "import os\n",
    "from pathlib import Path\n",
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "\n",
    "class AllenAPI:\n",
    "    \"\"\"Singleton class for accessing Allen Brain Observatory API\"\"\"\n",
    "    \n",
    "    _instance = None\n",
    "    _boc = None  # Lazy-loaded BrainObservatoryCache instance\n",
    "\n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    @property\n",
    "    def boc(self):\n",
    "        \"\"\"Lazy-load BrainObservatoryCache only when accessed.\"\"\"\n",
    "        if self._boc is None:\n",
    "            allen_cache_path = os.environ.get('CAIM_ALLEN_CACHE_PATH')\n",
    "            if not allen_cache_path:\n",
    "                raise ValueError(\"AllenAPI requires a valid cache path. Set `CAIM_ALLEN_CACHE_PATH` in .env.\")\n",
    "\n",
    "            manifest_path = Path(allen_cache_path) / 'brain_observatory_manifest.json'\n",
    "            self._boc = BrainObservatoryCache(manifest_file=str(manifest_path))\n",
    "\n",
    "        return self._boc\n",
    "\n",
    "    def get_boc(self):\n",
    "        \"\"\"Retrieve the BrainObservatoryCache object, ensuring it is initialized.\"\"\"\n",
    "        return self.boc\n",
    "\n",
    "# Create a global instance so that all files can use it\n",
    "allen_api = AllenAPI()\n",
    "\n",
    "class AllenStimuliFetchStep(PipelineStep):\n",
    "    \"\"\"\n",
    "    Fetches data from the Allen Brain Observatory.\n",
    "    The session IDs are hard-coded since the stimuli are always the same.\n",
    "    \"\"\"\n",
    "    # Hard-coded sessions\n",
    "    SESSION_A = 501704220\n",
    "    SESSION_B = 501559087\n",
    "    SESSION_C = 501474098\n",
    "\n",
    "    def __init__(self, boc):\n",
    "        \"\"\"\n",
    "        :param boc: BrainObservatoryCache instance (via the AllenAPI singleton).\n",
    "        \"\"\"\n",
    "        self.boc = boc\n",
    "\n",
    "    def process(self, data):\n",
    "        \"\"\"\n",
    "        Expects data to be either None or have (container_id, session, stimulus).\n",
    "        We fetch a dictionary of raw stimuli arrays, store them in data['raw_data_dct'].\n",
    "        \"\"\"\n",
    "        if isinstance(data, tuple):\n",
    "            container_id, session, stimulus = data\n",
    "            data = {'container_id': container_id, 'session': session, 'stimulus': stimulus}\n",
    "        elif data is None:\n",
    "            data = {}\n",
    "\n",
    "        raw_data_dct = {}\n",
    "\n",
    "        movie_one_dataset = self.boc.get_ophys_experiment_data(self.SESSION_A)\n",
    "        raw_data_dct['natural_movie_one'] = movie_one_dataset.get_stimulus_template('natural_movie_one')\n",
    "\n",
    "        movie_two_dataset = self.boc.get_ophys_experiment_data(self.SESSION_C)\n",
    "        raw_data_dct['natural_movie_two'] = movie_two_dataset.get_stimulus_template('natural_movie_two')\n",
    "\n",
    "        movie_three_dataset = self.boc.get_ophys_experiment_data(self.SESSION_A)\n",
    "        raw_data_dct['natural_movie_three'] = movie_three_dataset.get_stimulus_template('natural_movie_three')\n",
    "\n",
    "        natural_images_dataset = self.boc.get_ophys_experiment_data(self.SESSION_B)\n",
    "        raw_data_dct['natural_scenes'] = natural_images_dataset.get_stimulus_template('natural_scenes')\n",
    "\n",
    "        data['raw_data_dct'] = raw_data_dct\n",
    "        return data\n",
    "\n",
    "class AnalysisPipeline:\n",
    "    \"\"\"Executes a series of PipelineStep objects in order.\"\"\"\n",
    "    \n",
    "    def __init__(self, steps):\n",
    "        self.steps = steps\n",
    "    \n",
    "    def run(self, data):\n",
    "        for step in self.steps:\n",
    "            data = step.process(data)\n",
    "        return data\n",
    "\n",
    "# Create a global instance so that all files can use it\n",
    "allen_api = AllenAPI()\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "    # F) Build pipeline with all steps\n",
    "boc = allen_api.get_boc()\n",
    "pipeline = AnalysisPipeline([\n",
    "        AllenStimuliFetchStep(boc)])\n",
    "\n",
    "raw_data_dct=pipeline.run(None)\n",
    "'''\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 918, 1174)\n"
     ]
    }
   ],
   "source": [
    "images=raw_data_dct['raw_data_dct']['natural_scenes']\n",
    "\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[165. 165. 165. ...  69.  54.  54.]\n",
      "  [165. 165. 165. ...  69.  54.  54.]\n",
      "  [165. 165. 165. ...  69.  54.  54.]\n",
      "  ...\n",
      "  [144. 144. 144. ... 161. 197. 197.]\n",
      "  [131. 131. 131. ... 231. 244. 244.]\n",
      "  [131. 131. 131. ... 231. 244. 244.]]\n",
      "\n",
      " [[ 77.  77.  77. ...  79.  95.  95.]\n",
      "  [ 77.  77.  77. ...  79.  95.  95.]\n",
      "  [ 77.  77.  77. ...  79.  95.  95.]\n",
      "  ...\n",
      "  [ 63.  63.  63. ...  86.  87.  87.]\n",
      "  [ 70.  70.  70. ...  47.  51.  51.]\n",
      "  [ 70.  70.  70. ...  47.  51.  51.]]\n",
      "\n",
      " [[193. 193. 193. ... 178. 186. 186.]\n",
      "  [193. 193. 193. ... 178. 186. 186.]\n",
      "  [193. 193. 193. ... 178. 186. 186.]\n",
      "  ...\n",
      "  [176. 176. 176. ... 218. 215. 215.]\n",
      "  [173. 173. 173. ... 211. 211. 211.]\n",
      "  [173. 173. 173. ... 211. 211. 211.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 64.  64.  64. ...  33.  33.  34.]\n",
      "  [ 64.  64.  64. ...  33.  33.  34.]\n",
      "  [ 61.  61.  61. ...  35.  35.  33.]\n",
      "  ...\n",
      "  [128. 128. 129. ...  85.  85.  82.]\n",
      "  [128. 128. 129. ...  85.  85.  82.]\n",
      "  [123. 123. 129. ...  84.  84.  84.]]\n",
      "\n",
      " [[ 21.  21.  29. ... 107. 107. 112.]\n",
      "  [ 21.  21.  29. ... 107. 107. 112.]\n",
      "  [ 29.  29.  28. ... 113. 113. 116.]\n",
      "  ...\n",
      "  [135. 135. 134. ...  44.  44.  46.]\n",
      "  [135. 135. 134. ...  44.  44.  46.]\n",
      "  [134. 134. 139. ...  21.  21.  29.]]\n",
      "\n",
      " [[  0.   0.  13. ... 111. 111. 165.]\n",
      "  [  0.   0.  13. ... 111. 111. 165.]\n",
      "  [  0.   0.   0. ... 100. 100.  92.]\n",
      "  ...\n",
      "  [202. 202. 160. ...  93.  93.  66.]\n",
      "  [202. 202. 160. ...  93.  93.  66.]\n",
      "  [216. 216. 184. ...  95.  95.  57.]]]\n"
     ]
    }
   ],
   "source": [
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown bear, bruin, Ursus arctos\n",
      "brown bear, bruin, Ursus arctos\n",
      "brown bear, bruin, Ursus arctos\n",
      "mud turtle\n",
      "albatross, mollymawk\n",
      "lion, king of beasts, Panthera leo\n",
      "lion, king of beasts, Panthera leo\n",
      "lion, king of beasts, Panthera leo\n",
      "tusker\n",
      "tusker\n",
      "zebra\n",
      "tiger, Panthera tigris\n",
      "tiger, Panthera tigris\n",
      "cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\n",
      "coyote, prairie wolf, brush wolf, Canis latrans\n",
      "cheetah, chetah, Acinonyx jubatus\n",
      "leopard, Panthera pardus\n",
      "leopard, Panthera pardus\n",
      "bald eagle, American eagle, Haliaeetus leucocephalus\n",
      "kite\n",
      "canoe\n",
      "pelican\n",
      "leopard, Panthera pardus\n",
      "baboon\n",
      "hummingbird\n",
      "otter\n",
      "whiptail, whiptail lizard\n",
      "zebra\n",
      "quail\n",
      "red wolf, maned wolf, Canis rufus, Canis niger\n",
      "worm fence, snake fence, snake-rail fence, Virginia fence\n",
      "megalith, megalithic structure\n",
      "gazelle\n",
      "Siberian husky\n",
      "tusker\n",
      "kite\n",
      "ram, tup\n",
      "valley, vale\n",
      "hay\n",
      "Egyptian cat\n",
      "ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\n",
      "monarch, monarch butterfly, milkweed butterfly, Danaus plexippus\n",
      "ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\n",
      "steel arch bridge\n",
      "gazelle\n",
      "hartebeest\n",
      "bison\n",
      "lynx, catamount\n",
      "whiptail, whiptail lizard\n",
      "great grey owl, great gray owl, Strix nebulosa\n",
      "red wolf, maned wolf, Canis rufus, Canis niger\n",
      "bittern\n",
      "drake\n",
      "gorilla, Gorilla gorilla\n",
      "wallaby, brush kangaroo\n",
      "kite\n",
      "albatross, mollymawk\n",
      "goose\n",
      "great grey owl, great gray owl, Strix nebulosa\n",
      "park bench\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def classify_image(image):\n",
    "    # Convert NumPy image to PIL format\n",
    "    image_pil = Image.fromarray((image).astype(np.uint8))  # Ensure proper format\n",
    "\n",
    "    # Convert to PyTorch tensor and process\n",
    "    inputs = processor(images=image_pil, return_tensors=\"pt\")\n",
    "    \n",
    "    # Ensure model is on the same device as inputs\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    \n",
    "    return model.config.id2label[predicted_class_idx]\n",
    "\n",
    "# Convert grayscale to RGB\n",
    "images_rgb = np.repeat(images[:, :, :, np.newaxis], 3, axis=-1)  # Shape: (118, 918, 1174, 3)\n",
    "\n",
    "# Loop over images, classify, and display one at a time\n",
    "for i in range(118):\n",
    "    image = images_rgb[i]\n",
    "    \n",
    "    label = classify_image(image)\n",
    "    print(label)\n",
    "    \n",
    "    #plt.imshow(image / 255)  # Ensure correct range for display\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
