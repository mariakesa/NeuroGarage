{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maria/NeuroGarage/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from torch import nn\n",
    "import torch\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_spike_intervals(spike_times, start_times, stop_times):\n",
    "    spikes_in_intervals = {}\n",
    "\n",
    "    # Convert start_times and stop_times to numpy arrays for faster operations\n",
    "    start_times = np.array(start_times)\n",
    "    stop_times = np.array(stop_times)\n",
    "\n",
    "    print(start_times-stop_times)\n",
    "\n",
    "    # Loop through each neuron, but optimize the spike finding with vectorized operations\n",
    "    for neuron_id, times in spike_times.items():\n",
    "        # Convert times to a numpy array if it's not already\n",
    "        times = np.array(times)\n",
    "        \n",
    "        # Use numpy's searchsorted to find the indices where the start and stop times would fit\n",
    "        start_indices = np.searchsorted(times, start_times, side='left')\n",
    "        stop_indices = np.searchsorted(times, stop_times, side='right')\n",
    "        \n",
    "        # Get the number of spikes in each interval by subtracting indices\n",
    "        spikes_in_intervals[neuron_id] = stop_indices - start_indices\n",
    "    \n",
    "    return spikes_in_intervals\n",
    "\n",
    "\n",
    "# Sample LNP Model definition\n",
    "class LNPModel(nn.Module):\n",
    "    def __init__(self, input_dim, n_neurons):\n",
    "        super(LNPModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 10)  # Linear layer for each neuron\n",
    "        self.linear2=nn.Linear(10, n_neurons)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x=self.linear2(x)\n",
    "        firing_rate = torch.exp(x)  # Exponential non-linearity\n",
    "        return firing_rate\n",
    "\n",
    "class FrontierPipeline:\n",
    "    def __init__(self, session_id=831882777):\n",
    "        output_dir=os.environ['ALLEN_NEUROPIXELS_PATH']\n",
    "        manifest_path = os.path.join(output_dir, \"manifest.json\")\n",
    "        self.cache = EcephysProjectCache.from_warehouse(manifest=manifest_path)\n",
    "        self.session= self.cache.get_session_data(session_id)\n",
    "        self.stimuli_df=self.session.stimulus_presentations\n",
    "        embeddings=pickle.load(open('/home/maria/Documents/HuggingMouseData/TransformerEmbeddings/openai_clip-vit-base-patch32_embeddings.pkl','rb'))['natural_movie_one']\n",
    "        self.embeddings=torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "    def training_loop(self, lnp_model, real_spikes_tensor, trial_index):\n",
    "        # Training loop\n",
    "        # Loss function (Negative Log-Likelihood) and Optimizer\n",
    "        criterion = nn.PoissonNLLLoss(log_input=False)  # Poisson Negative Log-Likelihood Loss\n",
    "        optimizer = optim.Adam(lnp_model.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "        num_epochs = 10000\n",
    "        delta=0.0333\n",
    "        for epoch in range(num_epochs):\n",
    "            lnp_model.train()\n",
    "            \n",
    "            # Forward pass\n",
    "            predicted_firing_rate = lnp_model(self.embeddings).squeeze()*delta\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predicted_firing_rate, real_spikes_tensor)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print loss\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        print('Training finished')\n",
    "        weights=lnp_model.linear2.weight.detach().numpy()\n",
    "        return lnp_model, weights\n",
    "    \n",
    "    def __call__(self, trial_index, stimulus_type='natural_movie_one_more_repeats'):\n",
    "        stim = self.stimuli_df[self.stimuli_df['stimulus_name'] == stimulus_type]\n",
    "        spike_times=self.session.spike_times\n",
    "        spike_times=get_spike_intervals(spike_times,stim['start_time'].values,stim['stop_time'].values)\n",
    "        spikes=torch.tensor([spike_times[key] for key in spike_times.keys()],dtype=torch.float32)[:,trial_index*900:(trial_index+1)*900].T\n",
    "        lnp=LNPModel(self.embeddings.shape[1], len(spike_times.keys()))\n",
    "        lnp_model, weights=self.training_loop(lnp,spikes,trial_index)\n",
    "\n",
    "        return lnp_model, weights, self.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maria/NeuroGarage/.venv/lib/python3.10/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.1.3 because version 1.8.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n",
      "/home/maria/NeuroGarage/.venv/lib/python3.10/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'core' version 2.2.2 because version 2.7.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03336096 -0.03336096 -0.03336096 ... -0.0333607  -0.0333607\n",
      " -0.0333607 ]\n",
      "Epoch [10/10000], Loss: 0.7523\n",
      "Epoch [20/10000], Loss: 0.7319\n",
      "Epoch [30/10000], Loss: 0.7053\n",
      "Epoch [40/10000], Loss: 0.6709\n",
      "Epoch [50/10000], Loss: 0.6295\n",
      "Epoch [60/10000], Loss: 0.5849\n",
      "Epoch [70/10000], Loss: 0.5416\n",
      "Epoch [80/10000], Loss: 0.5023\n",
      "Epoch [90/10000], Loss: 0.4689\n",
      "Epoch [100/10000], Loss: 0.4412\n",
      "Epoch [110/10000], Loss: 0.4193\n",
      "Epoch [120/10000], Loss: 0.4035\n",
      "Epoch [130/10000], Loss: 0.3925\n",
      "Epoch [140/10000], Loss: 0.3850\n",
      "Epoch [150/10000], Loss: 0.3804\n",
      "Epoch [160/10000], Loss: 0.3778\n",
      "Epoch [170/10000], Loss: 0.3760\n",
      "Epoch [180/10000], Loss: 0.3746\n",
      "Epoch [190/10000], Loss: 0.3736\n",
      "Epoch [200/10000], Loss: 0.3729\n",
      "Epoch [210/10000], Loss: 0.3723\n",
      "Epoch [220/10000], Loss: 0.3719\n",
      "Epoch [230/10000], Loss: 0.3715\n",
      "Epoch [240/10000], Loss: 0.3712\n",
      "Epoch [250/10000], Loss: 0.3710\n",
      "Epoch [260/10000], Loss: 0.3709\n",
      "Epoch [270/10000], Loss: 0.3707\n",
      "Epoch [280/10000], Loss: 0.3706\n",
      "Epoch [290/10000], Loss: 0.3704\n",
      "Epoch [300/10000], Loss: 0.3703\n",
      "Epoch [310/10000], Loss: 0.3701\n",
      "Epoch [320/10000], Loss: 0.3700\n",
      "Epoch [330/10000], Loss: 0.3698\n",
      "Epoch [340/10000], Loss: 0.3696\n",
      "Epoch [350/10000], Loss: 0.3694\n",
      "Epoch [360/10000], Loss: 0.3691\n",
      "Epoch [370/10000], Loss: 0.3689\n",
      "Epoch [380/10000], Loss: 0.3686\n",
      "Epoch [390/10000], Loss: 0.3683\n",
      "Epoch [400/10000], Loss: 0.3681\n",
      "Epoch [410/10000], Loss: 0.3678\n",
      "Epoch [420/10000], Loss: 0.3676\n",
      "Epoch [430/10000], Loss: 0.3674\n",
      "Epoch [440/10000], Loss: 0.3672\n",
      "Epoch [450/10000], Loss: 0.3671\n",
      "Epoch [460/10000], Loss: 0.3669\n",
      "Epoch [470/10000], Loss: 0.3668\n",
      "Epoch [480/10000], Loss: 0.3667\n",
      "Epoch [490/10000], Loss: 0.3666\n",
      "Epoch [500/10000], Loss: 0.3666\n",
      "Epoch [510/10000], Loss: 0.3665\n",
      "Epoch [520/10000], Loss: 0.3665\n",
      "Epoch [530/10000], Loss: 0.3664\n",
      "Epoch [540/10000], Loss: 0.3664\n",
      "Epoch [550/10000], Loss: 0.3664\n",
      "Epoch [560/10000], Loss: 0.3664\n",
      "Epoch [570/10000], Loss: 0.3663\n",
      "Epoch [580/10000], Loss: 0.3663\n",
      "Epoch [590/10000], Loss: 0.3663\n",
      "Epoch [600/10000], Loss: 0.3663\n",
      "Epoch [610/10000], Loss: 0.3663\n",
      "Epoch [620/10000], Loss: 0.3663\n",
      "Epoch [630/10000], Loss: 0.3663\n",
      "Epoch [640/10000], Loss: 0.3662\n",
      "Epoch [650/10000], Loss: 0.3662\n",
      "Epoch [660/10000], Loss: 0.3662\n",
      "Epoch [670/10000], Loss: 0.3662\n",
      "Epoch [680/10000], Loss: 0.3662\n",
      "Epoch [690/10000], Loss: 0.3662\n",
      "Epoch [700/10000], Loss: 0.3662\n",
      "Epoch [710/10000], Loss: 0.3662\n",
      "Epoch [720/10000], Loss: 0.3662\n",
      "Epoch [730/10000], Loss: 0.3662\n",
      "Epoch [740/10000], Loss: 0.3662\n",
      "Epoch [750/10000], Loss: 0.3662\n",
      "Epoch [760/10000], Loss: 0.3662\n",
      "Epoch [770/10000], Loss: 0.3662\n",
      "Epoch [780/10000], Loss: 0.3662\n",
      "Epoch [790/10000], Loss: 0.3662\n",
      "Epoch [800/10000], Loss: 0.3662\n",
      "Epoch [810/10000], Loss: 0.3662\n",
      "Epoch [820/10000], Loss: 0.3662\n",
      "Epoch [830/10000], Loss: 0.3662\n",
      "Epoch [840/10000], Loss: 0.3662\n",
      "Epoch [850/10000], Loss: 0.3662\n",
      "Epoch [860/10000], Loss: 0.3662\n",
      "Epoch [870/10000], Loss: 0.3662\n",
      "Epoch [880/10000], Loss: 0.3662\n",
      "Epoch [890/10000], Loss: 0.3662\n",
      "Epoch [900/10000], Loss: 0.3662\n",
      "Epoch [910/10000], Loss: 0.3662\n",
      "Epoch [920/10000], Loss: 0.3662\n",
      "Epoch [930/10000], Loss: 0.3662\n",
      "Epoch [940/10000], Loss: 0.3662\n",
      "Epoch [950/10000], Loss: 0.3662\n",
      "Epoch [960/10000], Loss: 0.3662\n",
      "Epoch [970/10000], Loss: 0.3662\n",
      "Epoch [980/10000], Loss: 0.3662\n",
      "Epoch [990/10000], Loss: 0.3662\n",
      "Epoch [1000/10000], Loss: 0.3662\n",
      "Epoch [1010/10000], Loss: 0.3662\n",
      "Epoch [1020/10000], Loss: 0.3662\n",
      "Epoch [1030/10000], Loss: 0.3662\n",
      "Epoch [1040/10000], Loss: 0.3662\n",
      "Epoch [1050/10000], Loss: 0.3662\n",
      "Epoch [1060/10000], Loss: 0.3662\n",
      "Epoch [1070/10000], Loss: 0.3662\n",
      "Epoch [1080/10000], Loss: 0.3662\n",
      "Epoch [1090/10000], Loss: 0.3662\n",
      "Epoch [1100/10000], Loss: 0.3662\n",
      "Epoch [1110/10000], Loss: 0.3662\n",
      "Epoch [1120/10000], Loss: 0.3662\n",
      "Epoch [1130/10000], Loss: 0.3662\n",
      "Epoch [1140/10000], Loss: 0.3662\n",
      "Epoch [1150/10000], Loss: 0.3662\n",
      "Epoch [1160/10000], Loss: 0.3662\n",
      "Epoch [1170/10000], Loss: 0.3662\n",
      "Epoch [1180/10000], Loss: 0.3662\n",
      "Epoch [1190/10000], Loss: 0.3662\n",
      "Epoch [1200/10000], Loss: 0.3662\n",
      "Epoch [1210/10000], Loss: 0.3662\n",
      "Epoch [1220/10000], Loss: 0.3662\n",
      "Epoch [1230/10000], Loss: 0.3662\n",
      "Epoch [1240/10000], Loss: 0.3662\n",
      "Epoch [1250/10000], Loss: 0.3662\n",
      "Epoch [1260/10000], Loss: 0.3662\n",
      "Epoch [1270/10000], Loss: 0.3662\n",
      "Epoch [1280/10000], Loss: 0.3662\n",
      "Epoch [1290/10000], Loss: 0.3662\n",
      "Epoch [1300/10000], Loss: 0.3662\n",
      "Epoch [1310/10000], Loss: 0.3662\n",
      "Epoch [1320/10000], Loss: 0.3662\n",
      "Epoch [1330/10000], Loss: 0.3662\n",
      "Epoch [1340/10000], Loss: 0.3662\n",
      "Epoch [1350/10000], Loss: 0.3662\n",
      "Epoch [1360/10000], Loss: 0.3662\n",
      "Epoch [1370/10000], Loss: 0.3662\n",
      "Epoch [1380/10000], Loss: 0.3662\n",
      "Epoch [1390/10000], Loss: 0.3662\n",
      "Epoch [1400/10000], Loss: 0.3662\n",
      "Epoch [1410/10000], Loss: 0.3662\n",
      "Epoch [1420/10000], Loss: 0.3662\n",
      "Epoch [1430/10000], Loss: 0.3662\n",
      "Epoch [1440/10000], Loss: 0.3662\n",
      "Epoch [1450/10000], Loss: 0.3662\n",
      "Epoch [1460/10000], Loss: 0.3662\n",
      "Epoch [1470/10000], Loss: 0.3662\n",
      "Epoch [1480/10000], Loss: 0.3662\n",
      "Epoch [1490/10000], Loss: 0.3662\n",
      "Epoch [1500/10000], Loss: 0.3662\n",
      "Epoch [1510/10000], Loss: 0.3662\n",
      "Epoch [1520/10000], Loss: 0.3662\n",
      "Epoch [1530/10000], Loss: 0.3662\n",
      "Epoch [1540/10000], Loss: 0.3662\n",
      "Epoch [1550/10000], Loss: 0.3662\n",
      "Epoch [1560/10000], Loss: 0.3662\n",
      "Epoch [1570/10000], Loss: 0.3662\n",
      "Epoch [1580/10000], Loss: 0.3662\n",
      "Epoch [1590/10000], Loss: 0.3662\n",
      "Epoch [1600/10000], Loss: 0.3662\n",
      "Epoch [1610/10000], Loss: 0.3662\n",
      "Epoch [1620/10000], Loss: 0.3662\n",
      "Epoch [1630/10000], Loss: 0.3662\n",
      "Epoch [1640/10000], Loss: 0.3662\n",
      "Epoch [1650/10000], Loss: 0.3662\n",
      "Epoch [1660/10000], Loss: 0.3662\n",
      "Epoch [1670/10000], Loss: 0.3662\n",
      "Epoch [1680/10000], Loss: 0.3662\n",
      "Epoch [1690/10000], Loss: 0.3662\n",
      "Epoch [1700/10000], Loss: 0.3662\n",
      "Epoch [1710/10000], Loss: 0.3662\n",
      "Epoch [1720/10000], Loss: 0.3662\n",
      "Epoch [1730/10000], Loss: 0.3662\n",
      "Epoch [1740/10000], Loss: 0.3662\n",
      "Epoch [1750/10000], Loss: 0.3662\n",
      "Epoch [1760/10000], Loss: 0.3662\n",
      "Epoch [1770/10000], Loss: 0.3662\n",
      "Epoch [1780/10000], Loss: 0.3662\n",
      "Epoch [1790/10000], Loss: 0.3662\n",
      "Epoch [1800/10000], Loss: 0.3662\n",
      "Epoch [1810/10000], Loss: 0.3662\n",
      "Epoch [1820/10000], Loss: 0.3662\n",
      "Epoch [1830/10000], Loss: 0.3662\n",
      "Epoch [1840/10000], Loss: 0.3662\n",
      "Epoch [1850/10000], Loss: 0.3662\n",
      "Epoch [1860/10000], Loss: 0.3662\n",
      "Epoch [1870/10000], Loss: 0.3662\n",
      "Epoch [1880/10000], Loss: 0.3662\n",
      "Epoch [1890/10000], Loss: 0.3662\n",
      "Epoch [1900/10000], Loss: 0.3662\n",
      "Epoch [1910/10000], Loss: 0.3662\n",
      "Epoch [1920/10000], Loss: 0.3662\n",
      "Epoch [1930/10000], Loss: 0.3662\n",
      "Epoch [1940/10000], Loss: 0.3662\n",
      "Epoch [1950/10000], Loss: 0.3662\n",
      "Epoch [1960/10000], Loss: 0.3662\n",
      "Epoch [1970/10000], Loss: 0.3662\n",
      "Epoch [1980/10000], Loss: 0.3662\n",
      "Epoch [1990/10000], Loss: 0.3662\n",
      "Epoch [2000/10000], Loss: 0.3662\n",
      "Epoch [2010/10000], Loss: 0.3662\n",
      "Epoch [2020/10000], Loss: 0.3662\n",
      "Epoch [2030/10000], Loss: 0.3662\n",
      "Epoch [2040/10000], Loss: 0.3662\n",
      "Epoch [2050/10000], Loss: 0.3662\n",
      "Epoch [2060/10000], Loss: 0.3662\n",
      "Epoch [2070/10000], Loss: 0.3662\n",
      "Epoch [2080/10000], Loss: 0.3662\n",
      "Epoch [2090/10000], Loss: 0.3662\n",
      "Epoch [2100/10000], Loss: 0.3662\n",
      "Epoch [2110/10000], Loss: 0.3662\n",
      "Epoch [2120/10000], Loss: 0.3662\n",
      "Epoch [2130/10000], Loss: 0.3662\n",
      "Epoch [2140/10000], Loss: 0.3662\n",
      "Epoch [2150/10000], Loss: 0.3662\n",
      "Epoch [2160/10000], Loss: 0.3662\n",
      "Epoch [2170/10000], Loss: 0.3662\n",
      "Epoch [2180/10000], Loss: 0.3662\n",
      "Epoch [2190/10000], Loss: 0.3662\n",
      "Epoch [2200/10000], Loss: 0.3662\n",
      "Epoch [2210/10000], Loss: 0.3662\n",
      "Epoch [2220/10000], Loss: 0.3662\n",
      "Epoch [2230/10000], Loss: 0.3662\n",
      "Epoch [2240/10000], Loss: 0.3662\n",
      "Epoch [2250/10000], Loss: 0.3662\n",
      "Epoch [2260/10000], Loss: 0.3662\n",
      "Epoch [2270/10000], Loss: 0.3662\n",
      "Epoch [2280/10000], Loss: 0.3662\n",
      "Epoch [2290/10000], Loss: 0.3662\n",
      "Epoch [2300/10000], Loss: 0.3662\n",
      "Epoch [2310/10000], Loss: 0.3662\n",
      "Epoch [2320/10000], Loss: 0.3662\n",
      "Epoch [2330/10000], Loss: 0.3662\n",
      "Epoch [2340/10000], Loss: 0.3662\n",
      "Epoch [2350/10000], Loss: 0.3662\n",
      "Epoch [2360/10000], Loss: 0.3662\n",
      "Epoch [2370/10000], Loss: 0.3662\n",
      "Epoch [2380/10000], Loss: 0.3662\n",
      "Epoch [2390/10000], Loss: 0.3662\n",
      "Epoch [2400/10000], Loss: 0.3662\n",
      "Epoch [2410/10000], Loss: 0.3662\n",
      "Epoch [2420/10000], Loss: 0.3662\n",
      "Epoch [2430/10000], Loss: 0.3662\n",
      "Epoch [2440/10000], Loss: 0.3662\n",
      "Epoch [2450/10000], Loss: 0.3662\n",
      "Epoch [2460/10000], Loss: 0.3662\n",
      "Epoch [2470/10000], Loss: 0.3662\n",
      "Epoch [2480/10000], Loss: 0.3662\n",
      "Epoch [2490/10000], Loss: 0.3662\n",
      "Epoch [2500/10000], Loss: 0.3662\n",
      "Epoch [2510/10000], Loss: 0.3662\n",
      "Epoch [2520/10000], Loss: 0.3662\n",
      "Epoch [2530/10000], Loss: 0.3662\n",
      "Epoch [2540/10000], Loss: 0.3662\n",
      "Epoch [2550/10000], Loss: 0.3662\n",
      "Epoch [2560/10000], Loss: 0.3662\n",
      "Epoch [2570/10000], Loss: 0.3662\n",
      "Epoch [2580/10000], Loss: 0.3662\n",
      "Epoch [2590/10000], Loss: 0.3662\n",
      "Epoch [2600/10000], Loss: 0.3662\n",
      "Epoch [2610/10000], Loss: 0.3662\n",
      "Epoch [2620/10000], Loss: 0.3662\n",
      "Epoch [2630/10000], Loss: 0.3662\n",
      "Epoch [2640/10000], Loss: 0.3662\n",
      "Epoch [2650/10000], Loss: 0.3662\n",
      "Epoch [2660/10000], Loss: 0.3662\n",
      "Epoch [2670/10000], Loss: 0.3662\n",
      "Epoch [2680/10000], Loss: 0.3662\n",
      "Epoch [2690/10000], Loss: 0.3662\n",
      "Epoch [2700/10000], Loss: 0.3662\n",
      "Epoch [2710/10000], Loss: 0.3662\n",
      "Epoch [2720/10000], Loss: 0.3662\n",
      "Epoch [2730/10000], Loss: 0.3662\n",
      "Epoch [2740/10000], Loss: 0.3662\n",
      "Epoch [2750/10000], Loss: 0.3662\n",
      "Epoch [2760/10000], Loss: 0.3662\n",
      "Epoch [2770/10000], Loss: 0.3662\n",
      "Epoch [2780/10000], Loss: 0.3662\n",
      "Epoch [2790/10000], Loss: 0.3662\n",
      "Epoch [2800/10000], Loss: 0.3662\n",
      "Epoch [2810/10000], Loss: 0.3662\n",
      "Epoch [2820/10000], Loss: 0.3662\n",
      "Epoch [2830/10000], Loss: 0.3662\n",
      "Epoch [2840/10000], Loss: 0.3662\n",
      "Epoch [2850/10000], Loss: 0.3662\n",
      "Epoch [2860/10000], Loss: 0.3662\n",
      "Epoch [2870/10000], Loss: 0.3662\n",
      "Epoch [2880/10000], Loss: 0.3662\n",
      "Epoch [2890/10000], Loss: 0.3662\n",
      "Epoch [2900/10000], Loss: 0.3662\n",
      "Epoch [2910/10000], Loss: 0.3662\n",
      "Epoch [2920/10000], Loss: 0.3662\n",
      "Epoch [2930/10000], Loss: 0.3662\n",
      "Epoch [2940/10000], Loss: 0.3662\n",
      "Epoch [2950/10000], Loss: 0.3662\n",
      "Epoch [2960/10000], Loss: 0.3662\n",
      "Epoch [2970/10000], Loss: 0.3662\n",
      "Epoch [2980/10000], Loss: 0.3662\n",
      "Epoch [2990/10000], Loss: 0.3662\n",
      "Epoch [3000/10000], Loss: 0.3662\n",
      "Epoch [3010/10000], Loss: 0.3662\n",
      "Epoch [3020/10000], Loss: 0.3662\n",
      "Epoch [3030/10000], Loss: 0.3662\n",
      "Epoch [3040/10000], Loss: 0.3662\n",
      "Epoch [3050/10000], Loss: 0.3662\n",
      "Epoch [3060/10000], Loss: 0.3662\n",
      "Epoch [3070/10000], Loss: 0.3662\n",
      "Epoch [3080/10000], Loss: 0.3662\n",
      "Epoch [3090/10000], Loss: 0.3662\n",
      "Epoch [3100/10000], Loss: 0.3662\n",
      "Epoch [3110/10000], Loss: 0.3662\n",
      "Epoch [3120/10000], Loss: 0.3662\n",
      "Epoch [3130/10000], Loss: 0.3662\n",
      "Epoch [3140/10000], Loss: 0.3662\n",
      "Epoch [3150/10000], Loss: 0.3662\n",
      "Epoch [3160/10000], Loss: 0.3662\n",
      "Epoch [3170/10000], Loss: 0.3662\n",
      "Epoch [3180/10000], Loss: 0.3662\n",
      "Epoch [3190/10000], Loss: 0.3662\n",
      "Epoch [3200/10000], Loss: 0.3662\n",
      "Epoch [3210/10000], Loss: 0.3662\n",
      "Epoch [3220/10000], Loss: 0.3662\n",
      "Epoch [3230/10000], Loss: 0.3662\n",
      "Epoch [3240/10000], Loss: 0.3662\n",
      "Epoch [3250/10000], Loss: 0.3662\n",
      "Epoch [3260/10000], Loss: 0.3662\n",
      "Epoch [3270/10000], Loss: 0.3662\n",
      "Epoch [3280/10000], Loss: 0.3662\n",
      "Epoch [3290/10000], Loss: 0.3662\n",
      "Epoch [3300/10000], Loss: 0.3662\n",
      "Epoch [3310/10000], Loss: 0.3662\n",
      "Epoch [3320/10000], Loss: 0.3662\n",
      "Epoch [3330/10000], Loss: 0.3662\n",
      "Epoch [3340/10000], Loss: 0.3662\n",
      "Epoch [3350/10000], Loss: 0.3662\n",
      "Epoch [3360/10000], Loss: 0.3662\n",
      "Epoch [3370/10000], Loss: 0.3662\n",
      "Epoch [3380/10000], Loss: 0.3662\n",
      "Epoch [3390/10000], Loss: 0.3662\n",
      "Epoch [3400/10000], Loss: 0.3662\n",
      "Epoch [3410/10000], Loss: 0.3662\n",
      "Epoch [3420/10000], Loss: 0.3662\n",
      "Epoch [3430/10000], Loss: 0.3662\n",
      "Epoch [3440/10000], Loss: 0.3662\n",
      "Epoch [3450/10000], Loss: 0.3662\n",
      "Epoch [3460/10000], Loss: 0.3662\n",
      "Epoch [3470/10000], Loss: 0.3662\n",
      "Epoch [3480/10000], Loss: 0.3662\n",
      "Epoch [3490/10000], Loss: 0.3662\n",
      "Epoch [3500/10000], Loss: 0.3662\n",
      "Epoch [3510/10000], Loss: 0.3662\n",
      "Epoch [3520/10000], Loss: 0.3662\n",
      "Epoch [3530/10000], Loss: 0.3662\n",
      "Epoch [3540/10000], Loss: 0.3662\n",
      "Epoch [3550/10000], Loss: 0.3662\n",
      "Epoch [3560/10000], Loss: 0.3662\n",
      "Epoch [3570/10000], Loss: 0.3662\n",
      "Epoch [3580/10000], Loss: 0.3662\n",
      "Epoch [3590/10000], Loss: 0.3662\n",
      "Epoch [3600/10000], Loss: 0.3662\n",
      "Epoch [3610/10000], Loss: 0.3663\n",
      "Epoch [3620/10000], Loss: 0.3661\n",
      "Epoch [3630/10000], Loss: 0.3662\n",
      "Epoch [3640/10000], Loss: 0.3662\n",
      "Epoch [3650/10000], Loss: 0.3662\n",
      "Epoch [3660/10000], Loss: 0.3662\n",
      "Epoch [3670/10000], Loss: 0.3662\n",
      "Epoch [3680/10000], Loss: 0.3662\n",
      "Epoch [3690/10000], Loss: 0.3662\n",
      "Epoch [3700/10000], Loss: 0.3662\n",
      "Epoch [3710/10000], Loss: 0.3662\n",
      "Epoch [3720/10000], Loss: 0.3662\n",
      "Epoch [3730/10000], Loss: 0.3662\n",
      "Epoch [3740/10000], Loss: 0.3662\n",
      "Epoch [3750/10000], Loss: 0.3662\n",
      "Epoch [3760/10000], Loss: 0.3662\n",
      "Epoch [3770/10000], Loss: 0.3662\n",
      "Epoch [3780/10000], Loss: 0.3662\n",
      "Epoch [3790/10000], Loss: 0.3662\n",
      "Epoch [3800/10000], Loss: 0.3662\n",
      "Epoch [3810/10000], Loss: 0.3662\n",
      "Epoch [3820/10000], Loss: 0.3662\n",
      "Epoch [3830/10000], Loss: 0.3662\n",
      "Epoch [3840/10000], Loss: 0.3662\n",
      "Epoch [3850/10000], Loss: 0.3662\n",
      "Epoch [3860/10000], Loss: 0.3662\n",
      "Epoch [3870/10000], Loss: 0.3662\n",
      "Epoch [3880/10000], Loss: 0.3662\n",
      "Epoch [3890/10000], Loss: 0.3662\n",
      "Epoch [3900/10000], Loss: 0.3662\n",
      "Epoch [3910/10000], Loss: 0.3662\n",
      "Epoch [3920/10000], Loss: 0.3662\n",
      "Epoch [3930/10000], Loss: 0.3662\n",
      "Epoch [3940/10000], Loss: 0.3662\n",
      "Epoch [3950/10000], Loss: 0.3662\n",
      "Epoch [3960/10000], Loss: 0.3662\n",
      "Epoch [3970/10000], Loss: 0.3662\n",
      "Epoch [3980/10000], Loss: 0.3662\n",
      "Epoch [3990/10000], Loss: 0.3662\n",
      "Epoch [4000/10000], Loss: 0.3662\n",
      "Epoch [4010/10000], Loss: 0.3662\n",
      "Epoch [4020/10000], Loss: 0.3662\n",
      "Epoch [4030/10000], Loss: 0.3662\n",
      "Epoch [4040/10000], Loss: 0.3662\n",
      "Epoch [4050/10000], Loss: 0.3662\n",
      "Epoch [4060/10000], Loss: 0.3662\n",
      "Epoch [4070/10000], Loss: 0.3662\n",
      "Epoch [4080/10000], Loss: 0.3662\n",
      "Epoch [4090/10000], Loss: 0.3662\n",
      "Epoch [4100/10000], Loss: 0.3662\n",
      "Epoch [4110/10000], Loss: 0.3662\n",
      "Epoch [4120/10000], Loss: 0.3662\n",
      "Epoch [4130/10000], Loss: 0.3662\n",
      "Epoch [4140/10000], Loss: 0.3662\n",
      "Epoch [4150/10000], Loss: 0.3662\n",
      "Epoch [4160/10000], Loss: 0.3662\n",
      "Epoch [4170/10000], Loss: 0.3662\n",
      "Epoch [4180/10000], Loss: 0.3662\n",
      "Epoch [4190/10000], Loss: 0.3662\n",
      "Epoch [4200/10000], Loss: 0.3662\n",
      "Epoch [4210/10000], Loss: 0.3662\n",
      "Epoch [4220/10000], Loss: 0.3662\n",
      "Epoch [4230/10000], Loss: 0.3662\n",
      "Epoch [4240/10000], Loss: 0.3662\n",
      "Epoch [4250/10000], Loss: 0.3662\n",
      "Epoch [4260/10000], Loss: 0.3662\n",
      "Epoch [4270/10000], Loss: 0.3662\n",
      "Epoch [4280/10000], Loss: 0.3662\n",
      "Epoch [4290/10000], Loss: 0.3662\n",
      "Epoch [4300/10000], Loss: 0.3662\n",
      "Epoch [4310/10000], Loss: 0.3662\n",
      "Epoch [4320/10000], Loss: 0.3662\n",
      "Epoch [4330/10000], Loss: 0.3662\n",
      "Epoch [4340/10000], Loss: 0.3662\n",
      "Epoch [4350/10000], Loss: 0.3662\n",
      "Epoch [4360/10000], Loss: 0.3662\n",
      "Epoch [4370/10000], Loss: 0.3662\n",
      "Epoch [4380/10000], Loss: 0.3662\n",
      "Epoch [4390/10000], Loss: 0.3662\n",
      "Epoch [4400/10000], Loss: 0.3662\n",
      "Epoch [4410/10000], Loss: 0.3662\n",
      "Epoch [4420/10000], Loss: 0.3662\n",
      "Epoch [4430/10000], Loss: 0.3662\n",
      "Epoch [4440/10000], Loss: 0.3662\n",
      "Epoch [4450/10000], Loss: 0.3662\n",
      "Epoch [4460/10000], Loss: 0.3662\n",
      "Epoch [4470/10000], Loss: 0.3662\n",
      "Epoch [4480/10000], Loss: 0.3662\n",
      "Epoch [4490/10000], Loss: 0.3662\n",
      "Epoch [4500/10000], Loss: 0.3662\n",
      "Epoch [4510/10000], Loss: 0.3662\n",
      "Epoch [4520/10000], Loss: 0.3662\n",
      "Epoch [4530/10000], Loss: 0.3662\n",
      "Epoch [4540/10000], Loss: 0.3662\n",
      "Epoch [4550/10000], Loss: 0.3662\n",
      "Epoch [4560/10000], Loss: 0.3662\n",
      "Epoch [4570/10000], Loss: 0.3662\n",
      "Epoch [4580/10000], Loss: 0.3662\n",
      "Epoch [4590/10000], Loss: 0.3662\n",
      "Epoch [4600/10000], Loss: 0.3662\n",
      "Epoch [4610/10000], Loss: 0.3662\n",
      "Epoch [4620/10000], Loss: 0.3662\n",
      "Epoch [4630/10000], Loss: 0.3662\n",
      "Epoch [4640/10000], Loss: 0.3662\n",
      "Epoch [4650/10000], Loss: 0.3662\n",
      "Epoch [4660/10000], Loss: 0.3662\n",
      "Epoch [4670/10000], Loss: 0.3662\n",
      "Epoch [4680/10000], Loss: 0.3662\n",
      "Epoch [4690/10000], Loss: 0.3662\n",
      "Epoch [4700/10000], Loss: 0.3662\n",
      "Epoch [4710/10000], Loss: 0.3662\n",
      "Epoch [4720/10000], Loss: 0.3662\n",
      "Epoch [4730/10000], Loss: 0.3662\n",
      "Epoch [4740/10000], Loss: 0.3662\n",
      "Epoch [4750/10000], Loss: 0.3662\n",
      "Epoch [4760/10000], Loss: 0.3662\n",
      "Epoch [4770/10000], Loss: 0.3662\n",
      "Epoch [4780/10000], Loss: 0.3662\n",
      "Epoch [4790/10000], Loss: 0.3662\n",
      "Epoch [4800/10000], Loss: 0.3662\n",
      "Epoch [4810/10000], Loss: 0.3667\n",
      "Epoch [4820/10000], Loss: 0.3662\n",
      "Epoch [4830/10000], Loss: 0.3662\n",
      "Epoch [4840/10000], Loss: 0.3664\n",
      "Epoch [4850/10000], Loss: 0.3662\n",
      "Epoch [4860/10000], Loss: 0.3663\n",
      "Epoch [4870/10000], Loss: 0.3662\n",
      "Epoch [4880/10000], Loss: 0.3662\n",
      "Epoch [4890/10000], Loss: 0.3662\n",
      "Epoch [4900/10000], Loss: 0.3662\n",
      "Epoch [4910/10000], Loss: 0.3662\n",
      "Epoch [4920/10000], Loss: 0.3662\n",
      "Epoch [4930/10000], Loss: 0.3662\n",
      "Epoch [4940/10000], Loss: 0.3662\n",
      "Epoch [4950/10000], Loss: 0.3662\n",
      "Epoch [4960/10000], Loss: 0.3662\n",
      "Epoch [4970/10000], Loss: 0.3662\n",
      "Epoch [4980/10000], Loss: 0.3662\n",
      "Epoch [4990/10000], Loss: 0.3662\n",
      "Epoch [5000/10000], Loss: 0.3662\n",
      "Epoch [5010/10000], Loss: 0.3662\n",
      "Epoch [5020/10000], Loss: 0.3662\n",
      "Epoch [5030/10000], Loss: 0.3662\n",
      "Epoch [5040/10000], Loss: 0.3662\n",
      "Epoch [5050/10000], Loss: 0.3662\n",
      "Epoch [5060/10000], Loss: 0.3662\n",
      "Epoch [5070/10000], Loss: 0.3662\n",
      "Epoch [5080/10000], Loss: 0.3662\n",
      "Epoch [5090/10000], Loss: 0.3662\n",
      "Epoch [5100/10000], Loss: 0.3662\n",
      "Epoch [5110/10000], Loss: 0.3662\n",
      "Epoch [5120/10000], Loss: 0.3662\n",
      "Epoch [5130/10000], Loss: 0.3662\n",
      "Epoch [5140/10000], Loss: 0.3662\n",
      "Epoch [5150/10000], Loss: 0.3662\n",
      "Epoch [5160/10000], Loss: 0.3662\n",
      "Epoch [5170/10000], Loss: 0.3662\n",
      "Epoch [5180/10000], Loss: 0.3662\n",
      "Epoch [5190/10000], Loss: 0.3662\n",
      "Epoch [5200/10000], Loss: 0.3662\n",
      "Epoch [5210/10000], Loss: 0.3662\n",
      "Epoch [5220/10000], Loss: 0.3662\n",
      "Epoch [5230/10000], Loss: 0.3662\n",
      "Epoch [5240/10000], Loss: 0.3662\n",
      "Epoch [5250/10000], Loss: 0.3662\n",
      "Epoch [5260/10000], Loss: 0.3662\n",
      "Epoch [5270/10000], Loss: 0.3662\n",
      "Epoch [5280/10000], Loss: 0.3662\n",
      "Epoch [5290/10000], Loss: 0.3662\n",
      "Epoch [5300/10000], Loss: 0.3662\n",
      "Epoch [5310/10000], Loss: 0.3662\n",
      "Epoch [5320/10000], Loss: 0.3662\n",
      "Epoch [5330/10000], Loss: 0.3662\n",
      "Epoch [5340/10000], Loss: 0.3662\n",
      "Epoch [5350/10000], Loss: 0.3662\n",
      "Epoch [5360/10000], Loss: 0.3662\n",
      "Epoch [5370/10000], Loss: 0.3662\n",
      "Epoch [5380/10000], Loss: 0.3662\n",
      "Epoch [5390/10000], Loss: 0.3662\n",
      "Epoch [5400/10000], Loss: 0.3662\n",
      "Epoch [5410/10000], Loss: 0.3662\n",
      "Epoch [5420/10000], Loss: 0.3662\n",
      "Epoch [5430/10000], Loss: 0.3662\n",
      "Epoch [5440/10000], Loss: 0.3662\n",
      "Epoch [5450/10000], Loss: 0.3662\n",
      "Epoch [5460/10000], Loss: 0.3662\n",
      "Epoch [5470/10000], Loss: 0.3662\n",
      "Epoch [5480/10000], Loss: 0.3662\n",
      "Epoch [5490/10000], Loss: 0.3662\n",
      "Epoch [5500/10000], Loss: 0.3662\n",
      "Epoch [5510/10000], Loss: 0.3662\n",
      "Epoch [5520/10000], Loss: 0.3662\n",
      "Epoch [5530/10000], Loss: 0.3662\n",
      "Epoch [5540/10000], Loss: 0.3662\n",
      "Epoch [5550/10000], Loss: 0.3662\n",
      "Epoch [5560/10000], Loss: 0.3662\n",
      "Epoch [5570/10000], Loss: 0.3662\n",
      "Epoch [5580/10000], Loss: 0.3662\n",
      "Epoch [5590/10000], Loss: 0.3662\n",
      "Epoch [5600/10000], Loss: 0.3662\n",
      "Epoch [5610/10000], Loss: 0.3662\n",
      "Epoch [5620/10000], Loss: 0.3662\n",
      "Epoch [5630/10000], Loss: 0.3662\n",
      "Epoch [5640/10000], Loss: 0.3662\n",
      "Epoch [5650/10000], Loss: 0.3662\n",
      "Epoch [5660/10000], Loss: 0.3662\n",
      "Epoch [5670/10000], Loss: 0.3662\n",
      "Epoch [5680/10000], Loss: 0.3662\n",
      "Epoch [5690/10000], Loss: 0.3662\n",
      "Epoch [5700/10000], Loss: 0.3662\n",
      "Epoch [5710/10000], Loss: 0.3662\n",
      "Epoch [5720/10000], Loss: 0.3662\n",
      "Epoch [5730/10000], Loss: 0.3662\n",
      "Epoch [5740/10000], Loss: 0.3662\n",
      "Epoch [5750/10000], Loss: 0.3662\n",
      "Epoch [5760/10000], Loss: 0.3662\n",
      "Epoch [5770/10000], Loss: 0.3662\n",
      "Epoch [5780/10000], Loss: 0.3662\n",
      "Epoch [5790/10000], Loss: 0.3662\n",
      "Epoch [5800/10000], Loss: 0.3662\n",
      "Epoch [5810/10000], Loss: 0.3662\n",
      "Epoch [5820/10000], Loss: 0.3662\n",
      "Epoch [5830/10000], Loss: 0.3662\n",
      "Epoch [5840/10000], Loss: 0.3662\n",
      "Epoch [5850/10000], Loss: 0.3662\n",
      "Epoch [5860/10000], Loss: 0.3662\n",
      "Epoch [5870/10000], Loss: 0.3662\n",
      "Epoch [5880/10000], Loss: 0.3662\n",
      "Epoch [5890/10000], Loss: 0.3662\n",
      "Epoch [5900/10000], Loss: 0.3662\n",
      "Epoch [5910/10000], Loss: 0.3662\n",
      "Epoch [5920/10000], Loss: 0.3662\n",
      "Epoch [5930/10000], Loss: 0.3662\n",
      "Epoch [5940/10000], Loss: 0.3662\n",
      "Epoch [5950/10000], Loss: 0.3662\n",
      "Epoch [5960/10000], Loss: 0.3662\n",
      "Epoch [5970/10000], Loss: 0.3662\n",
      "Epoch [5980/10000], Loss: 0.3662\n",
      "Epoch [5990/10000], Loss: 0.3662\n",
      "Epoch [6000/10000], Loss: 0.3662\n",
      "Epoch [6010/10000], Loss: 0.3662\n",
      "Epoch [6020/10000], Loss: 0.3662\n",
      "Epoch [6030/10000], Loss: 0.3662\n",
      "Epoch [6040/10000], Loss: 0.3662\n",
      "Epoch [6050/10000], Loss: 0.3662\n",
      "Epoch [6060/10000], Loss: 0.3662\n",
      "Epoch [6070/10000], Loss: 0.3662\n",
      "Epoch [6080/10000], Loss: 0.3662\n",
      "Epoch [6090/10000], Loss: 0.3662\n",
      "Epoch [6100/10000], Loss: 0.3662\n",
      "Epoch [6110/10000], Loss: 0.3662\n",
      "Epoch [6120/10000], Loss: 0.3662\n",
      "Epoch [6130/10000], Loss: 0.3662\n",
      "Epoch [6140/10000], Loss: 0.3662\n",
      "Epoch [6150/10000], Loss: 0.3662\n",
      "Epoch [6160/10000], Loss: 0.3662\n",
      "Epoch [6170/10000], Loss: 0.3662\n",
      "Epoch [6180/10000], Loss: 0.3662\n",
      "Epoch [6190/10000], Loss: 0.3662\n",
      "Epoch [6200/10000], Loss: 0.3662\n",
      "Epoch [6210/10000], Loss: 0.3662\n",
      "Epoch [6220/10000], Loss: 0.3662\n",
      "Epoch [6230/10000], Loss: 0.3662\n",
      "Epoch [6240/10000], Loss: 0.3662\n",
      "Epoch [6250/10000], Loss: 0.3662\n",
      "Epoch [6260/10000], Loss: 0.3662\n",
      "Epoch [6270/10000], Loss: 0.3662\n",
      "Epoch [6280/10000], Loss: 0.3662\n",
      "Epoch [6290/10000], Loss: 0.3662\n",
      "Epoch [6300/10000], Loss: 0.3662\n",
      "Epoch [6310/10000], Loss: 0.3662\n",
      "Epoch [6320/10000], Loss: 0.3662\n",
      "Epoch [6330/10000], Loss: 0.3662\n",
      "Epoch [6340/10000], Loss: 0.3662\n",
      "Epoch [6350/10000], Loss: 0.3662\n",
      "Epoch [6360/10000], Loss: 0.3662\n",
      "Epoch [6370/10000], Loss: 0.3661\n",
      "Epoch [6380/10000], Loss: 0.3664\n",
      "Epoch [6390/10000], Loss: 0.3661\n",
      "Epoch [6400/10000], Loss: 0.3662\n",
      "Epoch [6410/10000], Loss: 0.3664\n",
      "Epoch [6420/10000], Loss: 0.3663\n",
      "Epoch [6430/10000], Loss: 0.3662\n",
      "Epoch [6440/10000], Loss: 0.3662\n",
      "Epoch [6450/10000], Loss: 0.3662\n",
      "Epoch [6460/10000], Loss: 0.3662\n",
      "Epoch [6470/10000], Loss: 0.3662\n",
      "Epoch [6480/10000], Loss: 0.3662\n",
      "Epoch [6490/10000], Loss: 0.3662\n",
      "Epoch [6500/10000], Loss: 0.3662\n",
      "Epoch [6510/10000], Loss: 0.3662\n",
      "Epoch [6520/10000], Loss: 0.3662\n",
      "Epoch [6530/10000], Loss: 0.3662\n",
      "Epoch [6540/10000], Loss: 0.3662\n",
      "Epoch [6550/10000], Loss: 0.3662\n",
      "Epoch [6560/10000], Loss: 0.3662\n",
      "Epoch [6570/10000], Loss: 0.3662\n",
      "Epoch [6580/10000], Loss: 0.3662\n",
      "Epoch [6590/10000], Loss: 0.3662\n",
      "Epoch [6600/10000], Loss: 0.3662\n",
      "Epoch [6610/10000], Loss: 0.3662\n",
      "Epoch [6620/10000], Loss: 0.3662\n",
      "Epoch [6630/10000], Loss: 0.3662\n",
      "Epoch [6640/10000], Loss: 0.3662\n",
      "Epoch [6650/10000], Loss: 0.3662\n",
      "Epoch [6660/10000], Loss: 0.3662\n",
      "Epoch [6670/10000], Loss: 0.3662\n",
      "Epoch [6680/10000], Loss: 0.3662\n",
      "Epoch [6690/10000], Loss: 0.3662\n",
      "Epoch [6700/10000], Loss: 0.3662\n",
      "Epoch [6710/10000], Loss: 0.3662\n",
      "Epoch [6720/10000], Loss: 0.3662\n",
      "Epoch [6730/10000], Loss: 0.3662\n",
      "Epoch [6740/10000], Loss: 0.3662\n",
      "Epoch [6750/10000], Loss: 0.3662\n",
      "Epoch [6760/10000], Loss: 0.3662\n",
      "Epoch [6770/10000], Loss: 0.3662\n",
      "Epoch [6780/10000], Loss: 0.3662\n",
      "Epoch [6790/10000], Loss: 0.3662\n",
      "Epoch [6800/10000], Loss: 0.3662\n",
      "Epoch [6810/10000], Loss: 0.3662\n",
      "Epoch [6820/10000], Loss: 0.3662\n",
      "Epoch [6830/10000], Loss: 0.3662\n",
      "Epoch [6840/10000], Loss: 0.3662\n",
      "Epoch [6850/10000], Loss: 0.3662\n",
      "Epoch [6860/10000], Loss: 0.3662\n",
      "Epoch [6870/10000], Loss: 0.3662\n",
      "Epoch [6880/10000], Loss: 0.3662\n",
      "Epoch [6890/10000], Loss: 0.3662\n",
      "Epoch [6900/10000], Loss: 0.3662\n",
      "Epoch [6910/10000], Loss: 0.3662\n",
      "Epoch [6920/10000], Loss: 0.3662\n",
      "Epoch [6930/10000], Loss: 0.3662\n",
      "Epoch [6940/10000], Loss: 0.3662\n",
      "Epoch [6950/10000], Loss: 0.3662\n",
      "Epoch [6960/10000], Loss: 0.3662\n",
      "Epoch [6970/10000], Loss: 0.3662\n",
      "Epoch [6980/10000], Loss: 0.3662\n",
      "Epoch [6990/10000], Loss: 0.3662\n",
      "Epoch [7000/10000], Loss: 0.3662\n",
      "Epoch [7010/10000], Loss: 0.3662\n",
      "Epoch [7020/10000], Loss: 0.3662\n",
      "Epoch [7030/10000], Loss: 0.3662\n",
      "Epoch [7040/10000], Loss: 0.3662\n",
      "Epoch [7050/10000], Loss: 0.3662\n",
      "Epoch [7060/10000], Loss: 0.3662\n",
      "Epoch [7070/10000], Loss: 0.3662\n",
      "Epoch [7080/10000], Loss: 0.3662\n",
      "Epoch [7090/10000], Loss: 0.3662\n",
      "Epoch [7100/10000], Loss: 0.3662\n",
      "Epoch [7110/10000], Loss: 0.3662\n",
      "Epoch [7120/10000], Loss: 0.3662\n",
      "Epoch [7130/10000], Loss: 0.3662\n",
      "Epoch [7140/10000], Loss: 0.3662\n",
      "Epoch [7150/10000], Loss: 0.3662\n",
      "Epoch [7160/10000], Loss: 0.3662\n",
      "Epoch [7170/10000], Loss: 0.3662\n",
      "Epoch [7180/10000], Loss: 0.3662\n",
      "Epoch [7190/10000], Loss: 0.3662\n",
      "Epoch [7200/10000], Loss: 0.3662\n",
      "Epoch [7210/10000], Loss: 0.3662\n",
      "Epoch [7220/10000], Loss: 0.3662\n",
      "Epoch [7230/10000], Loss: 0.3662\n",
      "Epoch [7240/10000], Loss: 0.3662\n",
      "Epoch [7250/10000], Loss: 0.3662\n",
      "Epoch [7260/10000], Loss: 0.3662\n",
      "Epoch [7270/10000], Loss: 0.3662\n",
      "Epoch [7280/10000], Loss: 0.3662\n",
      "Epoch [7290/10000], Loss: 0.3662\n",
      "Epoch [7300/10000], Loss: 0.3662\n",
      "Epoch [7310/10000], Loss: 0.3662\n",
      "Epoch [7320/10000], Loss: 0.3662\n",
      "Epoch [7330/10000], Loss: 0.3662\n",
      "Epoch [7340/10000], Loss: 0.3662\n",
      "Epoch [7350/10000], Loss: 0.3662\n",
      "Epoch [7360/10000], Loss: 0.3662\n",
      "Epoch [7370/10000], Loss: 0.3662\n",
      "Epoch [7380/10000], Loss: 0.3662\n",
      "Epoch [7390/10000], Loss: 0.3662\n",
      "Epoch [7400/10000], Loss: 0.3662\n",
      "Epoch [7410/10000], Loss: 0.3662\n",
      "Epoch [7420/10000], Loss: 0.3662\n",
      "Epoch [7430/10000], Loss: 0.3662\n",
      "Epoch [7440/10000], Loss: 0.3662\n",
      "Epoch [7450/10000], Loss: 0.3662\n",
      "Epoch [7460/10000], Loss: 0.3662\n",
      "Epoch [7470/10000], Loss: 0.3662\n",
      "Epoch [7480/10000], Loss: 0.3662\n",
      "Epoch [7490/10000], Loss: 0.3662\n",
      "Epoch [7500/10000], Loss: 0.3662\n",
      "Epoch [7510/10000], Loss: 0.3662\n",
      "Epoch [7520/10000], Loss: 0.3662\n",
      "Epoch [7530/10000], Loss: 0.3662\n",
      "Epoch [7540/10000], Loss: 0.3662\n",
      "Epoch [7550/10000], Loss: 0.3662\n",
      "Epoch [7560/10000], Loss: 0.3662\n",
      "Epoch [7570/10000], Loss: 0.3662\n",
      "Epoch [7580/10000], Loss: 0.3662\n",
      "Epoch [7590/10000], Loss: 0.3661\n",
      "Epoch [7600/10000], Loss: 0.3661\n",
      "Epoch [7610/10000], Loss: 0.3662\n",
      "Epoch [7620/10000], Loss: 0.3663\n",
      "Epoch [7630/10000], Loss: 0.3663\n",
      "Epoch [7640/10000], Loss: 0.3662\n",
      "Epoch [7650/10000], Loss: 0.3662\n",
      "Epoch [7660/10000], Loss: 0.3662\n",
      "Epoch [7670/10000], Loss: 0.3662\n",
      "Epoch [7680/10000], Loss: 0.3662\n",
      "Epoch [7690/10000], Loss: 0.3662\n",
      "Epoch [7700/10000], Loss: 0.3662\n",
      "Epoch [7710/10000], Loss: 0.3662\n",
      "Epoch [7720/10000], Loss: 0.3662\n",
      "Epoch [7730/10000], Loss: 0.3662\n",
      "Epoch [7740/10000], Loss: 0.3662\n",
      "Epoch [7750/10000], Loss: 0.3662\n",
      "Epoch [7760/10000], Loss: 0.3662\n",
      "Epoch [7770/10000], Loss: 0.3662\n",
      "Epoch [7780/10000], Loss: 0.3662\n",
      "Epoch [7790/10000], Loss: 0.3662\n",
      "Epoch [7800/10000], Loss: 0.3662\n",
      "Epoch [7810/10000], Loss: 0.3662\n",
      "Epoch [7820/10000], Loss: 0.3662\n",
      "Epoch [7830/10000], Loss: 0.3662\n",
      "Epoch [7840/10000], Loss: 0.3662\n",
      "Epoch [7850/10000], Loss: 0.3662\n",
      "Epoch [7860/10000], Loss: 0.3662\n",
      "Epoch [7870/10000], Loss: 0.3662\n",
      "Epoch [7880/10000], Loss: 0.3662\n",
      "Epoch [7890/10000], Loss: 0.3662\n",
      "Epoch [7900/10000], Loss: 0.3662\n",
      "Epoch [7910/10000], Loss: 0.3662\n",
      "Epoch [7920/10000], Loss: 0.3662\n",
      "Epoch [7930/10000], Loss: 0.3662\n",
      "Epoch [7940/10000], Loss: 0.3662\n",
      "Epoch [7950/10000], Loss: 0.3662\n",
      "Epoch [7960/10000], Loss: 0.3662\n",
      "Epoch [7970/10000], Loss: 0.3662\n",
      "Epoch [7980/10000], Loss: 0.3662\n",
      "Epoch [7990/10000], Loss: 0.3662\n",
      "Epoch [8000/10000], Loss: 0.3662\n",
      "Epoch [8010/10000], Loss: 0.3662\n",
      "Epoch [8020/10000], Loss: 0.3662\n",
      "Epoch [8030/10000], Loss: 0.3662\n",
      "Epoch [8040/10000], Loss: 0.3662\n",
      "Epoch [8050/10000], Loss: 0.3664\n",
      "Epoch [8060/10000], Loss: 0.3669\n",
      "Epoch [8070/10000], Loss: 0.3666\n",
      "Epoch [8080/10000], Loss: 0.3664\n",
      "Epoch [8090/10000], Loss: 0.3663\n",
      "Epoch [8100/10000], Loss: 0.3663\n",
      "Epoch [8110/10000], Loss: 0.3663\n",
      "Epoch [8120/10000], Loss: 0.3662\n",
      "Epoch [8130/10000], Loss: 0.3662\n",
      "Epoch [8140/10000], Loss: 0.3662\n",
      "Epoch [8150/10000], Loss: 0.3662\n",
      "Epoch [8160/10000], Loss: 0.3662\n",
      "Epoch [8170/10000], Loss: 0.3662\n",
      "Epoch [8180/10000], Loss: 0.3662\n",
      "Epoch [8190/10000], Loss: 0.3662\n",
      "Epoch [8200/10000], Loss: 0.3662\n",
      "Epoch [8210/10000], Loss: 0.3662\n",
      "Epoch [8220/10000], Loss: 0.3662\n",
      "Epoch [8230/10000], Loss: 0.3662\n",
      "Epoch [8240/10000], Loss: 0.3662\n",
      "Epoch [8250/10000], Loss: 0.3662\n",
      "Epoch [8260/10000], Loss: 0.3662\n",
      "Epoch [8270/10000], Loss: 0.3662\n",
      "Epoch [8280/10000], Loss: 0.3662\n",
      "Epoch [8290/10000], Loss: 0.3662\n",
      "Epoch [8300/10000], Loss: 0.3662\n",
      "Epoch [8310/10000], Loss: 0.3662\n",
      "Epoch [8320/10000], Loss: 0.3662\n",
      "Epoch [8330/10000], Loss: 0.3662\n",
      "Epoch [8340/10000], Loss: 0.3662\n",
      "Epoch [8350/10000], Loss: 0.3662\n",
      "Epoch [8360/10000], Loss: 0.3662\n",
      "Epoch [8370/10000], Loss: 0.3662\n",
      "Epoch [8380/10000], Loss: 0.3662\n",
      "Epoch [8390/10000], Loss: 0.3662\n",
      "Epoch [8400/10000], Loss: 0.3662\n",
      "Epoch [8410/10000], Loss: 0.3662\n",
      "Epoch [8420/10000], Loss: 0.3662\n",
      "Epoch [8430/10000], Loss: 0.3662\n",
      "Epoch [8440/10000], Loss: 0.3662\n",
      "Epoch [8450/10000], Loss: 0.3662\n",
      "Epoch [8460/10000], Loss: 0.3662\n",
      "Epoch [8470/10000], Loss: 0.3662\n",
      "Epoch [8480/10000], Loss: 0.3662\n",
      "Epoch [8490/10000], Loss: 0.3662\n",
      "Epoch [8500/10000], Loss: 0.3662\n",
      "Epoch [8510/10000], Loss: 0.3662\n",
      "Epoch [8520/10000], Loss: 0.3662\n",
      "Epoch [8530/10000], Loss: 0.3662\n",
      "Epoch [8540/10000], Loss: 0.3662\n",
      "Epoch [8550/10000], Loss: 0.3662\n",
      "Epoch [8560/10000], Loss: 0.3662\n",
      "Epoch [8570/10000], Loss: 0.3662\n",
      "Epoch [8580/10000], Loss: 0.3662\n",
      "Epoch [8590/10000], Loss: 0.3662\n",
      "Epoch [8600/10000], Loss: 0.3662\n",
      "Epoch [8610/10000], Loss: 0.3662\n",
      "Epoch [8620/10000], Loss: 0.3662\n",
      "Epoch [8630/10000], Loss: 0.3662\n",
      "Epoch [8640/10000], Loss: 0.3662\n",
      "Epoch [8650/10000], Loss: 0.3662\n",
      "Epoch [8660/10000], Loss: 0.3662\n",
      "Epoch [8670/10000], Loss: 0.3662\n",
      "Epoch [8680/10000], Loss: 0.3662\n",
      "Epoch [8690/10000], Loss: 0.3662\n",
      "Epoch [8700/10000], Loss: 0.3662\n",
      "Epoch [8710/10000], Loss: 0.3662\n",
      "Epoch [8720/10000], Loss: 0.3662\n",
      "Epoch [8730/10000], Loss: 0.3662\n",
      "Epoch [8740/10000], Loss: 0.3662\n",
      "Epoch [8750/10000], Loss: 0.3662\n",
      "Epoch [8760/10000], Loss: 0.3662\n",
      "Epoch [8770/10000], Loss: 0.3662\n",
      "Epoch [8780/10000], Loss: 0.3662\n",
      "Epoch [8790/10000], Loss: 0.3662\n",
      "Epoch [8800/10000], Loss: 0.3662\n",
      "Epoch [8810/10000], Loss: 0.3662\n",
      "Epoch [8820/10000], Loss: 0.3662\n",
      "Epoch [8830/10000], Loss: 0.3662\n",
      "Epoch [8840/10000], Loss: 0.3662\n",
      "Epoch [8850/10000], Loss: 0.3662\n",
      "Epoch [8860/10000], Loss: 0.3662\n",
      "Epoch [8870/10000], Loss: 0.3662\n",
      "Epoch [8880/10000], Loss: 0.3662\n",
      "Epoch [8890/10000], Loss: 0.3662\n",
      "Epoch [8900/10000], Loss: 0.3662\n",
      "Epoch [8910/10000], Loss: 0.3662\n",
      "Epoch [8920/10000], Loss: 0.3662\n",
      "Epoch [8930/10000], Loss: 0.3662\n",
      "Epoch [8940/10000], Loss: 0.3662\n",
      "Epoch [8950/10000], Loss: 0.3662\n",
      "Epoch [8960/10000], Loss: 0.3662\n",
      "Epoch [8970/10000], Loss: 0.3662\n",
      "Epoch [8980/10000], Loss: 0.3662\n",
      "Epoch [8990/10000], Loss: 0.3662\n",
      "Epoch [9000/10000], Loss: 0.3662\n",
      "Epoch [9010/10000], Loss: 0.3662\n",
      "Epoch [9020/10000], Loss: 0.3662\n",
      "Epoch [9030/10000], Loss: 0.3662\n",
      "Epoch [9040/10000], Loss: 0.3662\n",
      "Epoch [9050/10000], Loss: 0.3662\n",
      "Epoch [9060/10000], Loss: 0.3661\n",
      "Epoch [9070/10000], Loss: 0.3661\n",
      "Epoch [9080/10000], Loss: 0.3663\n",
      "Epoch [9090/10000], Loss: 0.3663\n",
      "Epoch [9100/10000], Loss: 0.3662\n",
      "Epoch [9110/10000], Loss: 0.3662\n",
      "Epoch [9120/10000], Loss: 0.3662\n",
      "Epoch [9130/10000], Loss: 0.3662\n",
      "Epoch [9140/10000], Loss: 0.3662\n",
      "Epoch [9150/10000], Loss: 0.3662\n",
      "Epoch [9160/10000], Loss: 0.3662\n",
      "Epoch [9170/10000], Loss: 0.3662\n",
      "Epoch [9180/10000], Loss: 0.3662\n",
      "Epoch [9190/10000], Loss: 0.3662\n",
      "Epoch [9200/10000], Loss: 0.3662\n",
      "Epoch [9210/10000], Loss: 0.3662\n",
      "Epoch [9220/10000], Loss: 0.3662\n",
      "Epoch [9230/10000], Loss: 0.3662\n",
      "Epoch [9240/10000], Loss: 0.3662\n",
      "Epoch [9250/10000], Loss: 0.3662\n",
      "Epoch [9260/10000], Loss: 0.3662\n",
      "Epoch [9270/10000], Loss: 0.3662\n",
      "Epoch [9280/10000], Loss: 0.3662\n",
      "Epoch [9290/10000], Loss: 0.3662\n",
      "Epoch [9300/10000], Loss: 0.3662\n",
      "Epoch [9310/10000], Loss: 0.3662\n",
      "Epoch [9320/10000], Loss: 0.3662\n",
      "Epoch [9330/10000], Loss: 0.3662\n",
      "Epoch [9340/10000], Loss: 0.3662\n",
      "Epoch [9350/10000], Loss: 0.3662\n",
      "Epoch [9360/10000], Loss: 0.3662\n",
      "Epoch [9370/10000], Loss: 0.3662\n",
      "Epoch [9380/10000], Loss: 0.3662\n",
      "Epoch [9390/10000], Loss: 0.3662\n",
      "Epoch [9400/10000], Loss: 0.3662\n",
      "Epoch [9410/10000], Loss: 0.3662\n",
      "Epoch [9420/10000], Loss: 0.3662\n",
      "Epoch [9430/10000], Loss: 0.3662\n",
      "Epoch [9440/10000], Loss: 0.3662\n",
      "Epoch [9450/10000], Loss: 0.3662\n",
      "Epoch [9460/10000], Loss: 0.3662\n",
      "Epoch [9470/10000], Loss: 0.3662\n",
      "Epoch [9480/10000], Loss: 0.3662\n",
      "Epoch [9490/10000], Loss: 0.3662\n",
      "Epoch [9500/10000], Loss: 0.3663\n",
      "Epoch [9510/10000], Loss: 0.3666\n",
      "Epoch [9520/10000], Loss: 0.3663\n",
      "Epoch [9530/10000], Loss: 0.3661\n",
      "Epoch [9540/10000], Loss: 0.3662\n",
      "Epoch [9550/10000], Loss: 0.3663\n",
      "Epoch [9560/10000], Loss: 0.3662\n",
      "Epoch [9570/10000], Loss: 0.3662\n",
      "Epoch [9580/10000], Loss: 0.3662\n",
      "Epoch [9590/10000], Loss: 0.3662\n",
      "Epoch [9600/10000], Loss: 0.3662\n",
      "Epoch [9610/10000], Loss: 0.3662\n",
      "Epoch [9620/10000], Loss: 0.3662\n",
      "Epoch [9630/10000], Loss: 0.3662\n",
      "Epoch [9640/10000], Loss: 0.3662\n",
      "Epoch [9650/10000], Loss: 0.3662\n",
      "Epoch [9660/10000], Loss: 0.3662\n",
      "Epoch [9670/10000], Loss: 0.3662\n",
      "Epoch [9680/10000], Loss: 0.3662\n",
      "Epoch [9690/10000], Loss: 0.3662\n",
      "Epoch [9700/10000], Loss: 0.3662\n",
      "Epoch [9710/10000], Loss: 0.3662\n",
      "Epoch [9720/10000], Loss: 0.3662\n",
      "Epoch [9730/10000], Loss: 0.3662\n",
      "Epoch [9740/10000], Loss: 0.3662\n",
      "Epoch [9750/10000], Loss: 0.3662\n",
      "Epoch [9760/10000], Loss: 0.3662\n",
      "Epoch [9770/10000], Loss: 0.3662\n",
      "Epoch [9780/10000], Loss: 0.3662\n",
      "Epoch [9790/10000], Loss: 0.3662\n",
      "Epoch [9800/10000], Loss: 0.3662\n",
      "Epoch [9810/10000], Loss: 0.3662\n",
      "Epoch [9820/10000], Loss: 0.3662\n",
      "Epoch [9830/10000], Loss: 0.3662\n",
      "Epoch [9840/10000], Loss: 0.3662\n",
      "Epoch [9850/10000], Loss: 0.3662\n",
      "Epoch [9860/10000], Loss: 0.3662\n",
      "Epoch [9870/10000], Loss: 0.3662\n",
      "Epoch [9880/10000], Loss: 0.3662\n",
      "Epoch [9890/10000], Loss: 0.3662\n",
      "Epoch [9900/10000], Loss: 0.3662\n",
      "Epoch [9910/10000], Loss: 0.3662\n",
      "Epoch [9920/10000], Loss: 0.3662\n",
      "Epoch [9930/10000], Loss: 0.3662\n",
      "Epoch [9940/10000], Loss: 0.3662\n",
      "Epoch [9950/10000], Loss: 0.3662\n",
      "Epoch [9960/10000], Loss: 0.3663\n",
      "Epoch [9970/10000], Loss: 0.3661\n",
      "Epoch [9980/10000], Loss: 0.3661\n",
      "Epoch [9990/10000], Loss: 0.3662\n",
      "Epoch [10000/10000], Loss: 0.3663\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "pipeline=FrontierPipeline()\n",
    "for i in range(1):\n",
    "    lnp_model, weights, embeddings=pipeline(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([900, 657])\n",
      "(900,)\n",
      "(768, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 175559.08 ,  -33034.2  ,  -69427.055, ..., -150790.61 ,\n",
       "         -83411.59 ,    8923.429],\n",
       "       [ -33034.2  , 2963195.2  , 1673984.4  , ...,  872260.8  ,\n",
       "        1611708.9  , 1435821.5  ],\n",
       "       [ -69427.055, 1673984.4  , 1144863.6  , ...,  658209.   ,\n",
       "         974887.9  ,  862380.1  ],\n",
       "       ...,\n",
       "       [-150790.61 ,  872260.8  ,  658209.   , ...,  788467.9  ,\n",
       "         619481.44 ,  530116.25 ],\n",
       "       [ -83411.59 , 1611708.9  ,  974887.9  , ...,  619481.44 ,\n",
       "        1015648.9  ,  810554.75 ],\n",
       "       [   8923.429, 1435821.5  ,  862380.1  , ...,  530116.25 ,\n",
       "         810554.75 ,  847222.06 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_Fisher_matrix(lnp_model, weights, embeddings):\n",
    "    firing_rate=lnp_model(embeddings)\n",
    "    print(firing_rate.shape)\n",
    "    delta=0.0333\n",
    "    take_neuron_0=firing_rate[:,0].detach().numpy()*delta\n",
    "    print(take_neuron_0.shape)\n",
    "    #Shape 768x900\n",
    "    emb_matrix=embeddings.detach().numpy().T@embeddings.detach().numpy()\n",
    "    #Shape 768x768-- embedding shape\n",
    "    # Reshape take_neuron_0 to broadcast along the embedding matrix dimensions\n",
    "    take_neuron_0 = take_neuron_0.reshape(-1, 1, 1)  # Shape (900, 1, 1)\n",
    "    \n",
    "    # Broadcast to create the scaled embedding matrix for each observation\n",
    "    fisher_matrix = take_neuron_0 * emb_matrix  # Shape (900, 768, 768)\n",
    "    \n",
    "    # Sum across the 0 axis to get the Fisher information matrix\n",
    "    fisher_matrix = fisher_matrix.sum(axis=0)  # Shape (768, 768)\n",
    "    print(fisher_matrix.shape)  # Expected shape (768, 768)\n",
    "    \n",
    "    return fisher_matrix\n",
    "\n",
    "calculate_Fisher_matrix(lnp_model, weights, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
