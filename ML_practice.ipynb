{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=np.array([-1,-1])\n",
    "x2=np.array([1,0])\n",
    "x3=np.array([-1,1.5])\n",
    "x3=np.array([-1,10])\n",
    "\n",
    "y1=1\n",
    "y2=-1\n",
    "y3=1\n",
    "\n",
    "x=np.array([x1,x2,x3])\n",
    "y=np.array([y1,y2,y3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0]\n",
      "converged\n"
     ]
    }
   ],
   "source": [
    "#Perceptron learning rule\n",
    "w=np.array([0,0])\n",
    "x=np.array([x2,x3,x1])\n",
    "y=np.array([y2,y3,y1])\n",
    "\n",
    "for i in range(100):\n",
    "    for j in range(len(x)):\n",
    "        if y[j]*(np.dot(w,x[j]))<=0:\n",
    "            w=w+y[j]*x[j]\n",
    "            print(w)\n",
    "            break\n",
    "    else:\n",
    "        print(\"converged\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1]\n",
      "[-2  9]\n",
      "[-3  8]\n",
      "[-4  7]\n",
      "[-5  6]\n",
      "[-6  5]\n",
      "Converged\n",
      "1\n",
      "6\n",
      "56\n",
      "[-6  5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = np.array([-1, -1])\n",
    "x2 = np.array([1, 0])\n",
    "x3 = np.array([-1, 10])\n",
    "\n",
    "y1 = 1\n",
    "y2 = -1\n",
    "y3 = 1\n",
    "\n",
    "x = np.array([x1, x2, x3])\n",
    "y = np.array([y1, y2, y3])\n",
    "#x=np.array([x2, x3, x1])\n",
    "#y=np.array([y2, y3, y1])\n",
    "w = np.array([0, 0])\n",
    "\n",
    "for i in range(100):\n",
    "    for j in range(len(x)):\n",
    "        if y[j] * (np.dot(w, x[j])) <= 0:\n",
    "            w = w + y[j] * x[j]\n",
    "            print(w)\n",
    "            break\n",
    "    else:\n",
    "        print(\"Converged\")\n",
    "        break\n",
    "\n",
    "print(y[0] * (np.dot(w, x[0])))\n",
    "\n",
    "print(y[1] * (np.dot(w, x[1])))\n",
    "\n",
    "print(y[2] * (np.dot(w, x[2])))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results after gradient descent:\n",
      "  w = [0.         0.02350538],   b = 0.9999999999999999\n",
      "  Final loss = 0.000138\n",
      "  Check margin = w2 + b = 1.023505\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def svm_single_example_loss_and_grad(w, b):\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "      - L: the SVM loss for a single example x=[0,1], y=+1, lambda=0.5\n",
    "      - grad_w: gradient w.r.t. w = (w1,w2)\n",
    "      - grad_b: gradient w.r.t. b\n",
    "    \"\"\"\n",
    "    # Regularization term = 0.25 * (w1^2 + w2^2)\n",
    "    reg = 0.25 * np.sum(w * w)\n",
    "    \n",
    "    # Check hinge condition: 1 - (w2 + b)\n",
    "    margin_expr = 1 - (w[1] + b)\n",
    "    \n",
    "    if margin_expr <= 0:\n",
    "        # Region A: hinge = 0\n",
    "        L = reg\n",
    "        grad_w = 0.5 * w    # derivative of 0.25*(w^2) is 0.5*w\n",
    "        grad_b = 0.0\n",
    "    else:\n",
    "        # Region B: hinge > 0 => 1 - (w2 + b)\n",
    "        hinge = margin_expr\n",
    "        L = reg + hinge\n",
    "        # gradient of reg = (0.5*w1, 0.5*w2)\n",
    "        # gradient of hinge w.r.t. w2 = -1, w1 is unaffected by hinge\n",
    "        grad_w = np.array([0.5 * w[0], 0.5 * w[1] - 1])\n",
    "        grad_b = -1.0\n",
    "    \n",
    "    return L, grad_w, grad_b\n",
    "\n",
    "\n",
    "# --- Gradient Descent Hyperparameters ---\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "\n",
    "# --- Initialization ---\n",
    "w = np.array([0.0, 0.0])\n",
    "b = 0.0\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    L, grad_w, grad_b = svm_single_example_loss_and_grad(w, b)\n",
    "    \n",
    "    # Update parameters\n",
    "    w = w - learning_rate * grad_w\n",
    "    b = b - learning_rate * grad_b\n",
    "    \n",
    "    # (Optional) print progress\n",
    "    # print(f\"Iter={i}, w={w}, b={b:.3f}, L={L:.4f}\")\n",
    "\n",
    "# Final check\n",
    "L_final, _, _ = svm_single_example_loss_and_grad(w, b)\n",
    "print(\"Final results after gradient descent:\")\n",
    "print(f\"  w = {w},   b = {b}\")\n",
    "print(f\"  Final loss = {L_final:.6f}\")\n",
    "print(f\"  Check margin = w2 + b = {w[1] + b:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
